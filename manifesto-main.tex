% Emerald Publishing - Construction Innovation Submission Template
% by Oleksandr Melnyk
% Ver 0.0.4
% Based on: https://www.emeraldgrouppublishing.com/journal/ci#author-guidelines

\documentclass[11pt]{article}

\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
%\usepackage[a4paper,top=1.5cm,bottom=1.5cm,left=1.5cm,right=1.5cm,marginparwidth=1.75cm]{geometry}
\usepackage[a4paper,top=2cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=2.25cm]{geometry}

% Useful packages
\usepackage{amssymb}
\usepackage{siunitx}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{cleveref}
\usepackage[utf8]{inputenc}
\usepackage[right]{lineno}
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{url}
\usepackage{titlesec}
%\usepackage[compatibility=false]{caption}
\usepackage{authblk}
\usepackage{xcolor} % Load the xcolor package for color options
\usepackage{microtype}

%\usepackage{makecell} % multiline table cells

\renewcommand{\thetable}{\Roman{table}}

% Define a new format for \subsection
\titleformat{\subsection}
  {\mdseries\itshape\large} % Medium series, italic shape, and large font size
  {\thesubsection}{1em}{} % Numbering, spacing, and the section title itself


% Emerald Harvard Citation Style

\usepackage[english]{babel}
\usepackage[style=authoryear,backend=biber,natbib=true,maxcitenames=2,uniquelist=false]{biblatex}
%\usepackage{natbib}
\addbibresource{manifesto-main.bib} % your .bib file

% % Customizing biblatex for Harvard style
% % Customizing biblatex for Harvard style
% \DeclareNameAlias{sortname}{family-given}
% \DeclareNameAlias{default}{family-given}

\renewbibmacro{in:}{}
% \DeclareFieldFormat[article]{title}{\mkbibquote{#1}\addcomma}
% \DeclareFieldFormat[book]{title}{\mkbibemph{#1}\addcomma}
% \DeclareFieldFormat[bookinbook]{title}{\mkbibemph{#1}\addcomma}
% \DeclareFieldFormat[inbook]{title}{\mkbibquote{#1}\addcomma}
% \DeclareFieldFormat[incollection]{title}{\mkbibquote{#1}\addcomma}
% \DeclareFieldFormat[inproceedings]{title}{\mkbibquote{#1}\addcomma}
% \DeclareFieldFormat[manual]{title}{\mkbibemph{#1}\addcomma}
% \DeclareFieldFormat[misc]{title}{\mkbibemph{#1}\addcomma}
% \DeclareFieldFormat[thesis]{title}{\mkbibemph{#1}\addcomma}
% \DeclareFieldFormat[unpublished]{title}{\mkbibquote{#1}\addcomma}
% \DeclareFieldFormat[patent]{title}{\mkbibemph{#1}\addcomma}
% \DeclareFieldFormat[report]{title}{\mkbibemph{#1}\addcomma}
% \DeclareFieldFormat[online]{title}{\mkbibquote{#1}\addcomma}
% \DeclareFieldFormat[software]{title}{\mkbibemph{#1}\addcomma}
% \DeclareFieldFormat[booklet]{title}{\mkbibemph{#1}\addcomma}
% \DeclareFieldFormat[periodical]{title}{\mkbibemph{#1}\addcomma}
% \DeclareFieldFormat[standard]{title}{\mkbibemph{#1}\addcomma}

% \DeclareFieldFormat[article]{journaltitle}{\iffieldundef{shortjournal}{\mkbibemph{#1}\addcomma}{\mkbibemph{\printfield{shortjournal}}\addcomma}}
% \DeclareFieldFormat{volume}{\bibstring{volume}~#1}
% \DeclareFieldFormat{number}{\bibstring{number}~#1}

% Definitions for "Vol." and "No."
\DefineBibliographyStrings{english}{
  volume = {Vol.},
  number = {No.}
}

\renewbibmacro*{volume+number+eid}{%
  \printfield{volume}%
  \setunit*{\addspace}%
  \printfield{number}%
  \setunit{\addcomma\space}%
  \printfield{eid}}

\renewbibmacro*{journal+issuetitle}{%
  \usebibmacro{journal}%
  \setunit*{\addcomma\space}%
  \usebibmacro{volume+number+eid}%
  \setunit{\addcomma\space}%
  \usebibmacro{issue+date}}

\renewbibmacro*{publisher+location+date}{%
  \printlist{publisher}%
  \iflistundef{location}
    {\setunit*{\addcomma\space}}
    {\setunit*{\addcolon\space}}%
  \printlist{location}%
  \setunit*{\addcomma\space}%
  \usebibmacro{date}}

\renewcommand*{\bibpagespunct}{\addcomma\space}

% Customizing page field format to prevent duplication
% \DeclareFieldFormat{pages}{%
%   \mkfirstpage[{\mkpageprefix[page]{#1}}]{#1}}

% Customizing citations for Harvard style
\DeclareCiteCommand{\cite}[\mkbibparens]
  {\usebibmacro{prenote}}
  {\usebibmacro{citeindex}%
   \usebibmacro{cite}}
  {\multicitedelim}
  {\usebibmacro{postnote}}

\renewbibmacro*{cite:labelyear+extrayear}{%
  \iffieldundef{labelyear}
    {}
    {\printtext[bibhyperref]{%
       \printfield{labelyear}%
       \printfield{extrayear}}}}

\renewbibmacro*{cite:labeldate+extradate}{%
  \iffieldundef{labelyear}
    {}
    {\printtext[bibhyperref]{%
       \printfield{labelyear}%
       \printfield{extradate}}}}

\AtEveryBibitem{
  \clearfield{month}
  \clearfield{day}
  \ifentrytype{book}{
    \clearlist{location}
  }{}
}

% Formatting "et al." in italics followed by a comma
\DefineBibliographyStrings{english}{
  andothers = {\textit{et al.},}
}

\DeclareFieldFormat[article]{volume}{\bibstring{jourvol}\addnbspace #1}
\DeclareFieldFormat[article]{number}{\bibstring{number}\addnbspace #1}
\DeclareFieldFormat[article]{volume}{Vol. #1}
\DeclareFieldFormat[article]{number}{No. #1}
% Customizing DOI field format to lowercase "doi"
%\DeclareFieldFormat{doi}{\bibstring{doi}\addcolon\space\url{#1}}

% Customizing URL field format to "available at:"
\DeclareFieldFormat{url}{\bibstring{available at}\addcolon\space\url{#1}}
\DeclareFieldFormat{urldate}{\mkbibparens{accessed \addspace#1}}

% Customizing urldate to match the required format
\DeclareFieldFormat{urldate}{%
  \mkbibparens{accessed\space%
    \thefield{urlday}\addspace%
    \mkbibmonth{\thefield{urlmonth}}\addspace%
    \thefield{urlyear}}}


% Configure cleveref
\crefformat{figure}{#2Figure~#1#3}
\Crefformat{figure}{#2Figure~#1#3}
\crefformat{table}{#2Table~#1#3}
\Crefformat{table}{#2Table~#1#3}
\crefformat{section}{#2Section~#1#3}
\Crefformat{section}{#2Section~#1#3}


\title{Building a continuous benchmarking ecosystem in bioinformatics}

\author[1,2,*]{Izaskun Mallona}
\author[1,2,3]{Almut Lütge}
\author[2,4]{Charlotte Soneson}
\author[1]{Ben Carrillo}
\author[1,2]{Reto Gerber}
\author[1]{Daniel Incicau}
\author[1,2]{Anthony Sonrel}
\author[1,2,*]{Mark D. Robinson}

\affil[1]{Department of Molecular Life Sciences, University of Zurich, 8057 Zurich, Switzerland}
\affil[2]{SIB Swiss Institute of Bioinformatics, University of Zurich, 8057 Zurich, Switzerland}
\affil[3]{Swiss Data Science Centre, 8092 Zurich, Switzerland}
\affil[4]{Friedrich Miescher Institute for Biomedical Research, 4056 Basel, Switzerland}

\affil[*]{\{mark.robinson,izaskun.mallona\}@mls.uzh.ch}

\linespread{1.4} 

\begin{document}
\maketitle

\linenumbers


\begin{abstract}
Benchmarking, which involves collecting reference datasets and demonstrating method performances, is a requirement for the development of new computational tools, but also becomes a domain of its own to achieve neutral comparisons of methods. Although a lot has been written about how to design and conduct benchmark studies, this Perspective sheds light on a wish list for a computational platform to orchestrate benchmark studies. We discuss various ideas for organizing reproducible software environments, formally defining benchmarks, orchestrating standardized workflows, and how they interface with computing infrastructure.
\end{abstract}

%add 6 keywords
\textbf{Keywords:} benchmarking; bioinformatics; reproducibility; software environments; licensing; workflows; infrastructure


\pagebreak

\section*{Introduction}
\label{sec:introduction}

In this work, we give our perspective on how to build a high-standard benchmarking ecosystem in the field of bioinformatics, coming primarily from the viewpoint of methodologists (i.e., researchers with computational background working on tools for data). In particular, our case is one of computational scientists embedded in data-rich biology (i.e., computational biology and bioinformatics), where one of the distinguishing factors is rapid technology development (e.g., single-cell assays coupled to high-throughput sequencing, imaging, etc.), which in turn results in a rapid response from the computational community to develop tools, models, and software for the analysis of such emerging datasets. Although our viewpoint is `bioinformatics', there are analogs in adjacent disciplines where methodologists are creating computational tools in a certain scientific context. One critical step in method development is benchmarking (further defined below), whereby either new methods are compared against existing ones, or a set of existing methods are compared in a more neutral way; for these, we will make use of the terms (and abbreviations) `methods-development papers' (MDPs) and `benchmark-only papers' (BOPs), respectively, defined in a recent meta-analysis study \cite{Cao2023-jz}. Notably, there are various ways to structure the running of a benchmarking study: it can be run by a single person or a small group on a laptop (or a server), or it can be organized as a `challenge' or within the duration of a hackathon (groups of people that come together to competitively assess their approaches and/or populate benchmark components).

Our goal here is to lay out general benchmark parameters and definitions, while abstracting towards an ideal \textit{benchmarking system}. This ranges from general benchmark conceptualization to various technical specifications. We will offer some remarks on the philosophy around the benchmarking activity (e.g., gatekeeping), as well as some technical recommendations (e.g., standards).


\section{What is a benchmark?}
%\label{sec:main_section}

To begin our discussion of benchmarking (in bioinformatics), a few definitions are needed. For example, what actually is a benchmark, precisely? Here, a benchmark is thought of as a conceptual entity that roughly encompasses all the components of a study to understand the performance of a set of (computational) methods for a given task (here, the primary context is computational methods, but the concept can have a broader scope, e.g., evaluation of data-generating technologies). A benchmark requires a well-defined task, such as inferring what set of genes are differentially expressed from a certain type of data (e.g., gene expression from RNA sequencing counts) or whether a high-dimensional dataset can be clustered into the `correct' cell types (the definition of this correctness, or ground-truth, should be precisely defined in advance). Given a well-defined task, a benchmark consists of benchmark `components' (such as simulations to generate datasets, preprocessing steps, methods, metrics). Ideally, a benchmarking system is used to organize and orchestrate the running of the benchmark, and create `artifacts' (code snapshots, file outputs to be shared, results tables). Ultimately, the downstream results and interpretations, which may involve rankings and various additional analyses, will be derived from these artifacts. The various elements of code, especially the details on how datasets are preprocessed and how methods are run, should be explicitly available for scrutiny. Such a system should facilitate components to be public, open and allow contributions (e.g. corrections or new components), such that the benchmarks of the future can run more continuously.

At the outset of a benchmark, a benchmark `definition' could be envisioned, which could give a formal specification of the entire set of components (and the pattern of artifacts to be generated). The benchmark definition could even be expressed as a single configuration file that specifies the scope and topology of components to include, details of the repositories (code implementations with versions), the instructions to create software environments (accessible across compute architectures), and the parameters used. A benchmark definition in terms of layers and their corresponding challenges and opportunities within each layer is illustrated in Figure~\ref{fig:layers}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/bens_layers.pdf}
    \caption[Benchmarking conceptual layers and their challenges.]{Benchmarking conceptual layers and their challenges. The process of benchmarking is multilayered, with layer-specific factors belonging to different categories (colored boxes) and challenges (in red). \emph{Hardware}: infrastructure and its costs. \emph{Data}: dataset archival, openness, interoperability, archival, and selection.  \emph{Software}: method implementations, reproducibility, workflow execution, continuous integration and delivery (CI/CD), versioning, quality assurance (QA). \emph{Community}: Standardization and impartiality (governance, transparency, building trust, long term maintainability). \emph{Knowledge}: research and meta-research, academic publications.}
    \label{fig:layers}
\end{figure}

\section*{Benchmarks stakeholders}

Benchmarks are useful for a broad set of stakeholders. For a \emph{data analyst}, with an aim to perform a specific type of analysis on a specific dataset, benchmarks are helpful tools to find a suitable method for the task. However, it is important that the benchmarks consider datasets that are similar to the one of interest to the analyst, as method performance often depends on characteristics of the data. Thus, a thorough characterization of the datasets considered in a benchmark is desirable. Given that benchmarks typically evaluate methods using a range of metrics and datasets, not all of which are relevant to all analysts, flexibility in defining the ranking approach and aggregation of metrics is preferable. Such flexibility, which requires access to various artifacts, is typically not part of published benchmarks today \cite{Sonrel2023-te}. However, a well-structured benchmarking system could accommodate flexible filtering and aggregation of metrics and datasets, and additionally provide access to the code and software stack that the analyst would need to apply the selected method to their own dataset. 

For a \emph{method developer}, benchmarking serves the purpose of comparing their method to the state of the art in the field. Ideally, this should be done in a way that is as neutral as possible, avoiding selecting datasets and metrics that would unfairly favor the new method \cite{Boulesteix2013-vy, Weber2019-el}. A diverse, easily accessible, collection of benchmarking datasets and metrics for a given task would be one way of addressing this, and additionally has the benefit of lowering the barrier of entry for method developers into a field of application. Moreover, it is important that the new method is compared to the current state of competing methods, to best mimic the choices available to a potential method user. One could, however, imagine other use cases, where the new method would be systematically compared to previous versions of itself or alternative implementations of the `same' method \cite{Rich2024-jh}, to illustrate stability of code changes or improvements made over time. In both cases, the typical approach today would be for the method developer to rerun all the methods and versions to which they want to compare their current method, on all the datasets they wish to use. This can be challenging and time consuming, since not all methods run on the same hardware setup or in the same software environment (e.g., R and Python versions, library dependencies). It also leads to a lot of redundancy across method comparisons (multiple stakeholders implementing similar workflows), which could be reduced by the presence of a benchmarking system where such results would be accessible and extendable. Finally, the results of the benchmark of a new method should be included in the corresponding publication, and thus an easy way of including the new method into the set of results served by the benchmarking system, and generating a snapshot in time for reproducibility, would be of great help to the developer. 

Benchmarks can also be of high value to \emph{scientific journals and funding agencies}. Well-executed benchmark studies can be highly utilized, influential and cited, and may point to gaps in the current body of methods, guiding future methodological developments; at the same time, in fast-moving fields like bioinformatics, benchmarks have the tendency to rapidly become stale. Journals and funding bodies also have a vested interest in ensuring that published or funded method evaluations are performed to a high standard, that unnecessary redundancy is reduced, and that results are FAIR (findable, accessible, interoperable and reusable) for maximal benefit to the community \cite{Wilkinson2016-bh}. Thus, quality assurance, neutrality and transparency of benchmarking systems in themselves become highly relevant. 

Finally, benchmarks could form a research topic on their own, either for newcomers to the field to get acquainted with the conceptual options that are available, or for experts to consolidate research efforts of a subfield. For this, it is important that there are suitable venues for publishing benchmarks. Regardless, a \emph{benchmarker} (i.e., researcher leading a benchmarking study) would also benefit from using a benchmarking system and curated collections of benchmark artifacts. Such benchmarkers may be good candidates for curating and maintaining such collections by designing and leading benchmark efforts as well as guiding other contributors to adhere to a high standard. 

\section*{Benchmarks could be formally defined}

Benchmark artifacts can be seen as a collection of data files and (perhaps diverse) source code implementations with a pipeline topology (e.g., methods are run on some input data, and method outputs act as input for metrics) that were ideally run as a workflow in some computing environment. Current practices in single-cell RNA-seq benchmarking show that code is frequently open, reusable and versioned; input datasets are also typically available \cite{Sonrel2023-te}. However, the \textit{extensibility} of benchmarking studies is generally very low, as well as the proportion of benchmarks using a formal workflow system. Thus, a formal system to organize components and orchestrate the crossing of datasets, methods, and metrics would largely impact the current landscape.

In an ideal situation, running a benchmark includes executing a workflow, a process for which over 350 workflow languages, platforms or systems  \cite{Wratten2021-el, Amstutz2024-qk} and one standard named Common Workflow Language (CWL) (with multiple implementations) exist \cite{Amstutz2016-vo}. Some workflow languages can export workflow definitions to CWL, hence helping computational FAIR guidelines: that is, bundling and processing data in association to their metadata, tracking new metadata generation, recording provenance, and being accessible \cite{Goble2020-ps}. Both the workflow layout and provenance can be formally defined (Figure~\ref{fig:formalization}). 


\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/bens_formalization.pdf}
    \caption[Benchmark formalization.]{Benchmark formalization. \textbf{A}) Running a benchmark requires running a workflow (execution phase) to generate results to be critically evaluated (analysis phase). \textbf{B}) the execution phase consists on a mapping of methods to specific input files to generate output files, with optional steps to harmonize data and parametrize runs.}
    \label{fig:formalization}
\end{figure}

Benchmarking as a process deals with tasks other than workflow definition and execution, including: collecting and keeping track of contributions from users; provisioning or being aware of homogeneous and comparable hardware; handling a reproducible and efficient software stack; managing storage and their access-control lists; rendering results dashboards and summarizing results; versioning code, workflow runs, and files; and providing extensive documentation from defining scope or contribution guidelines, to transparent logging and crediting contributions and disseminating results. Benchmarking systems are in charge of orchestrating both workflow execution and these benchmarking-specific tasks.

Currently, most benchmarking systems aim to enhance the workflow formalization and execution with strategies to fulfill benchmarking-specific needs. To name a few of these systems, \texttt{ncbench} bundles workflow specification and software management with \texttt{snakemake} to benchmark result visualization with \texttt{Datavzrd} \cite{Hanssen2023-uf}. \texttt{OpenEBench} runs workflows in any workflow language, frequently \texttt{nextflow}, and makes use of openVRE as GUI and means of visualization \cite{Capella-Gutierrez2017-dh}. \texttt{OpenProblems} uses \texttt{Viash} and \texttt{nextflow} to handle software environments and workflows, respectively, and reports updated leaderboards with custom code \cite{Luecken2024-fk}. To our knowledge, no benchmarking platform makes use of a fully expressive benchmark definition formalization language covering both the workflow generation and the benchmarking-specific tasks. Such a language would go a long way toward facilitating benchmark conceptualization, community gathering and result sharing.

\section*{Scope and interpretation of benchmarking results}

Benchmarks are often focused on finding the `top' performing method, or on generating ranked lists of methods. While this is valid in a few fields with clearly defined goals, such as image recognition \cite{Khan2018-ot} or protein structure prediction \cite{Jumper2021-oq}, bioinformatics tasks are typically evaluated by multiple metrics, some of which are not well understood \cite{Lutge2021-mt, Reinke2024-dw}. Additionally, ground truths are not always well-defined (e.g., derived with uncertainty from other experimental data), multiple tasks (e.g., represented by different reference datasets) may be evaluated for a single method and several ways to assess performance are possible. Even when accompanied by dataset- and/or metric-specific performance measures, scaling and aggregation of results make it difficult to unpack fine-grained nuances of performance from a final summary table reported as a FunkyHeatmap \cite{funkyheatmap} as in most benchmarks (e.g., \cite{Saelens2019-jy}). Strobl \textit{et al.} push back against the `one method fits all datasets’ philosophy and advocate for highlighting dataset or parameter properties that are predictive of method performance \cite{Jelizarow2010-kr, Boulesteix2010-zj, Strobl2024-lp}. This aligns with what a data analyst wants: to identify the best performers in settings most similar to their problem at hand. This requires not only a thorough benchmark design, e.g., tracking of meaningful dataset metadata \cite{Strobl2024-lp}, but also can be supported by a benchmarking system that facilitates flexible navigation of results (e.g., via an interactive dashboard). 

Besides dashboards for more nuanced result exploration \cite{MariniUnknown-aq}, multi-criteria decision analysis (MCDA) \cite{Taherdoost2023-wd} has been proposed to guide users through benchmark results and multidimensional scaling (MDS) has been used to differentiate the effect of single datasets or metrics and recognize patterns beyond aggregated performance \cite{Niessl2022-fk}. An extensive analysis and presentation of the results not only helps the users in the long term but also the whole scientific field to identify pain points, the complexity of possible scenarios and to isolate the different use-cases. The first advantage of such a strategy is to force the user to an attentive reading of the performance results so that the correct use-case and relevant methods can be identified. The second, more important, advantage to present benchmark results extensively is to identify the challenges in a field of analysis, i.e., the scenarios, types of datasets, or aspects of performance that are systematically associated with poor results for all current methods. Being public and open to contributions from specialists of different fields, a benchmarking system could gather more reference datasets and methods, and allow the current state of the field to be better grasped. 

%As a result, the performance results will tend to better fit to the current state of the field, as opposed to a static benchmark led by a small panel of experts that leads to a smaller, and sometimes (un)intentionally biased view of the field \cite{Boulesteix2010-zj, Weber2019-el}. 


\section*{Parameter sweeps}

Parameters can be included into a benchmark to test method performance across a range of settings, allowing the impact of parameter choices to be assessed  or to find the optimal parameter settings. Notably, when assessing the impact of a certain parameter, the research objective of the benchmark differs from a classic method comparison: methods that do not include the parameter to be explored cannot be evaluated for this question. Parameters can be method-specific, but also dataset-specific, e.g., the true number of clusters when evaluating clustering algorithms. In this case, they can also be seen as (dataset-specific) metadata and the method performances can be compared depending on the availability of this metadata. A direct comparison between methods, with only some utilizing these metadata prevents differentiating between the effect of the parameter and the method itself for performance evaluation. Similarly, optimizing parameters for some but not other methods can skew results. Again, method-specific parameter tuning can be seen as a separate benchmarking task. Alternatively, a method that uses different settings can be interpreted as multiple distinct methods that are evaluated as part of the benchmark. This should be clearly communicated in the results. 

Altogether, evaluating parameters as part of a benchmark can give useful insights, but should be linked to a distinct and independent research objective.

For continuous and collaborative benchmarking, parameter tuning and usage should be governed and coordinated between all contributors. Ideally, the benchmarker specifies the parameter scope and related research objectives at the beginning of the benchmark to ensure comparability and interpretability of the results. 

\section*{Collaboration, gatekeeping and benchmarking infrastructure go hand in hand}

In designing a benchmarking system, it may be helpful to specify whether the benchmark is intended to be run `centralized' or `decentralized'. These terms can take different meanings depending on the context, but here the benchmark infrastructure location is intended. In most systems, the code to execute will be tracked in a reference \texttt{git} repository (e.g., on GitHub, GitLab). The workflow execution and the storage of derived datasets, and various stages of results are typically located either in local deployments (e.g., OEB-VRE for \texttt{OpenEBench}), commercial clouds (e.g., AWS for \texttt{OpenProblems}), commercial continuous integration and continuous delivery platforms (e.g., GitHub actions for \texttt{ncbench}) or could be flexible, e.g., on arbitrary infrastructure (i.e., could be a laptop, server or cloud). 

Traditionally, benchmarks (whether BOP or MDP) are conducted in a decentralized fashion and typically by a small number of researchers (e.g., for MDP, the new method developer). It is worth mentioning some disadvantages of the decentralized approach. For example, decentralization can be hugely inefficient; researchers studying the same task would collect reference datasets and code snippets to run methods, and implement a bespoke strategy (e.g., shell scripts) to orchestrate the methods run on datasets. Working together would mean sharing not only datasets and code snippets, but also execution environments (software but perhaps also hardware), alignment on parameters to evaluate and eventually sharing, consolidating, and interpreting results. However, because there are no standards of how benchmarks are implemented \cite{Sonrel2023-te}, another large disadvantage of decentralized benchmarks is that, while these components may eventually be openly shared as a product of a research study, the contents of a benchmark are non-interoperable, even across benchmarks of the same task. Furthermore, decentralized benchmarks have been criticized on the basis of `inflating model performance' due to decisions made in preprocessing or parameter tuning \cite{Luecken2024-fk}; such issues could be mitigated by gathering the `wisdom of crowds' that is implicit in running centralized benchmarks.

Arguably, most benchmarks could naturally start out as decentralized, in part as a means to properly define the concept and problem that one wants to work on, but also as a way to generate preliminary results and ideas to solidify the design before `going public' (centralization). Thus, users need a system that can build benchmark components in both a centralized and decentralized fashion, i.e., starting decentralized and promoting a benchmark to centralized when it becomes sufficiently mature. Accordingly, the process of centralization is not cost-free: it implies gatekeeping (so method contributors can authenticate to the system and be given access to computational resources) and adding measures to build trust and transparency.

\section*{Building trust and bringing together communities}

An active community is vital to the success of a benchmarking system. As mentioned, different roles and ways that people can contribute to or interact with a benchmarking system have been defined. The benchmarkers (which may include multiple scientists) are responsible for planning and coordinating a benchmark, defining the task, possibly splitting it into subtasks or processing stages, and defining the data formats across the stages; the benchmarker also brings an authority role of how to review and approve contributions. The contributors (which may include method developers) curate and add content to the benchmark, which could be new datasets, analysis methods or evaluation metrics, adhering to the guidelines set up by the benchmarker. Finally, the viewers (which may include benchmarkers and contributors) of benchmark results are users who retrieve one or more of the artifacts (dataset files, intermediate results, or metric scores). This could span a range of use cases, including the data analyst choosing which method to use for a specific application, an instructor retrieving a curated dataset for teaching purposes, or a methods researcher prototyping their new method.

Given the importance of such a community around a benchmarking system, the question arises on how it can best be assembled and sustained. Some strategies include organizing events, including seminars or workshops (typically on the educational side) as well as hackathons and challenges (more aimed at practical onboarding or solicitation of contributions). Career-related or financial incentives, such as the prospect of a scientific publication, or compensation for the allocated time, are also common motivators. For a method developer, contributing to a public benchmark provides a way to test their method without the need to set up all the infrastructure themselves, and also provides an opportunity to advertise their work. Ultimately, the likelihood that someone will contribute to a benchmark will likely be strongly driven by whether the results are of interest to them and their research, as well as how easy it is to contribute. Community engagement and contributions can further be made easier by providing clear guidelines (and organization), a suggestion of `good first issues' to tackle, a clear set of contributor guidelines and a code of conduct that covers all interactions related to the project.

The public solicitation of contributions, however, is not without difficulties, and it is important to organize the setup transparently to ensure trust from all parties. A benchmarker should be able to trace and test contributions, to detect misbehaviour and suspicious activities, but also to avoid that non-functional or suboptimal contributions enter the benchmark. This can be addressed by a `quarantine zone', where new content is automatically tested before being added to the system (i.e., executed in a sandbox environment). Similarly, a contributor should be able to trace their contribution, to ensure that no unintended modifications are made after submission. Ultimately, challenges arise when anyone can add a new method. Benchmarkers are typically not equally familiar with all included methods, and one would also need to be aware of potential malicious intent (e.g., submitting a suboptimal application of a competing method). On the other hand, many risks can be mitigated by the inherent transparency \cite{Greenstein2016-os}, and feedback loops with (all) original developers could be established to ensure optimal method usage.

A related challenge concerns which datasets and metrics to include in a benchmark. On the one hand, diverse sets of datasets and metrics are important to ensure generalizability and avoid overfitting. On the other hand, inclusion of low-quality or redundant datasets or metrics does not benefit the benchmark, and may provide misleading (aggregate) results. Possible mitigations may include sensitivity analyses, evaluating the impact of single datasets or metrics on the overall, aggregated performance of a method, or reducing redundancy by down-weighting similar metrics to avoid a specific aspect dominating the evaluation. From a wider perspective, these questions are all related to the governance of a benchmark, a topic that has not received much attention. Ultimately benchmarks may want to, depending on their scale and complexity, implement different governance models. In one model, the benchmarkers have the ultimate authority to decide on what gets included in the benchmark, simplifying the decision making but creating a risk of gatekeeping. In another model, decisions are made collectively by a group of contributors, which may mitigate the risk of gatekeeping at least partially, but creates a more complex decision structure and introduces the need for a conflict resolution strategy. The governance model of a benchmark could also cover aspects related to computing infrastructure and storage, as well as guidelines to manage authorship of any resulting publications or resources. Having the governance model spelled out when launching a benchmark is likely to reduce friction at later stages of the project. In all cases, an advantage of doing things in public is that contributions are logged. 

Neutrality is a highly desirable property in a benchmark: it means that efforts have been taken to ensure impartiality or unbiasedness (or at least transparency) of the various choices made (e.g., what datasets to include, how exactly to run methods, how to evaluate methods). Although highly desirable, neutrality is nonetheless extremely tough to assess given only the downstream artifacts of a benchmark; this is an area of governance that deserves further attention to increase trust. One could claim that all MDP benchmarks are non-neutral, since the goal is generally to highlight the virtues of a method. Similarly, ground truth generation and simulations in MDP benchmarks can unconsciously mirror the method capabilities, mechanisms, or strengths, thus biasing evaluation. Furthermore, there are many field experts who may wish to conduct a benchmark (or participate in a collaborative benchmark) on analyses that involve their own tools, where neutrality is then hard to establish (e.g., a perceived conflict of interest to evaluate one's own method) or where a deeper knowledge of their own tools might favor the implementation of the latter compared to other competitors. Here, some recent work to mitigate this conflict involves pre-registration, which typically involves specifying the parameters of your study in advance and posting them on a registry \cite{Sullivan2019-wh, Olevska2021-if}; initial formalization of a benchmark could semi-automatically provide the basis for the formal preregistration. In contrast, BOP benchmarks give, in principle, a higher degree of neutrality, because the goal is not simply to advertise the virtues of a single new method; nonetheless, the challenge to assess neutrality directly remains difficult. Other benchmarking models are possible, from challenges to hackathons and here, there is hope that a consensus of the masses can help to establish neutrality. One potential strategy, in the context of the systematic design of benchmarks, is establishing a system where a benchmark definition is parsed into a pre-registration template. Templates exist for simulation studies that could be adapted more generally for benchmarking projects \cite{Siepe2023-tz}. How to reconcile continuity and pre-registration of benchmarks needs further attention.

Finally, it is worth noting that there is a tension between preregistration and the concept of `continuous' benchmarking, where new datasets, methods and new metrics are added over time. The logic of continuous benchmarking is that as the problem becomes more understood in the field, it becomes increasingly clear what are good ways to evaluate the performance of methods. 

\section*{Long-term software reproducibility and data preservation}

Software plays an important role in the current scientific landscape \cite{Howison2015-rc}, since it is widely used throughout the scientific process, including data collection, simulation, analysis and reporting. Researchers are encouraged to publish their data and code to enhance transparency and reproducibility in their work. Ideally, this would increase benchmark (software) reuse by the wider scientific community. In practice, however, it is sometimes easier to develop new code than to reuse existing software \cite{Trisovic2022-ol}. This leads to the phenomenon of `academic abandonware', where projects are forgotten in code repositories, exacerbating the reproducibility crisis in research; one of the noted challenges in benchmarking was the lack of extensible frameworks \cite{Sonrel2023-te}. A common reason for abandoned projects is their failure to adhere to the FAIR (Findable, Accessible, Interoperable, Reusable) principles \cite{Wilkinson2016-bh}. Another common cause is the life cycle inherent to academic practice: after delivering a research output, there are often no resources to maintain a software package in the longer term; left alone, the likelihood of a particular library compiling or being able to run after 5-10 years will drastically decrease.

Operational costs are given by the expected compute and storage requirements. In the context of benchmarking, predicting CPU and memory usage is challenging. The methods under evaluation typically have variable computational demands. Benchmark designers could impose resource limits, thereby evaluating the methods under different constraints. Storage costs, on the other hand, are more predictable and can be directly influenced by design decisions of the benchmark (e.g., low retention for re-generatable artifacts, `cold storage' for code archives and software, external data stores such as Zenodo for method performance artifacts, CernVM-FS for reusable software).

\subsection*{Design properties}

A good benchmarking system needs to package its components in a way that ensures that their execution and artifacts can be consistently replicated within practical limits.

The UNIX design philosophy \cite{Pike1984-we} is worth embracing to produce software for benchmarking systems that are lightweight, robust, and easily maintainable; ease of use and extensibility are also desirable. Furthermore, to reduce the entry barrier for contributors, the benchmarking system should be capable of running locally with minimal setup. This approach ensures that the system remains relevant and functional even in the absence of centralization. Furthermore, intuitive APIs and sensible defaults \cite{Proctor2018-do} are factors that contribute to highly usable systems. Structured approaches to documentation, including adopting frameworks like diataxis \cite{ProcidaUnknown-vf}, can facilitate adoption and new contributions to a benchmark.

\subsection*{Reproducible software environments}

Software reproducibility comes from controlling the triad of data, code, and environment \cite{Hill2024-gf}. Computationally, a benchmark executes code that transforms input data into outputs: this execution takes place and is affected by the execution environment; the environment encompasses the base system (OS, compiler toolchains, libraries) and a set of configurations. The benchmark definition should control as much as possible of the execution environment.

Leaving data aside, it can be useful to divide the codebase used into three distinct categories:

\begin{enumerate}
\item The benchmarking system itself has an impact on operational costs, such as storage needs, execution platform, etc. For example, it imposes a certain software stack and base dependencies, and orchestrates the execution of a benchmark plan; usually, the system mandates the choice of a particular workflow manager. At this level, any requirements for input/output formats and shapes are defined; different systems can impose different constraints.

\item Contributions related to individual benchmark components (e.g., datasets, methods, metrics) are expected to be small, typically short scripts that process data and wrap functionality by importing external libraries (where the component implementation itself is developed). Even so, imposing good practices at this level (output validation, testing for abnormal termination, ability to run with a subset of input data) can be beneficial to increase the quality and maintainability of the contributions. The toolset can also enforce the validation of metadata annotations (authorship, versioning, licences) for each contributed module.

\item The software dependencies have easily the biggest impact on replicability and maintainability; archiving all combinations of a large dependency tree, across an arbitrary number of base OS images, will exponentially increase retention and maintenance costs. Turning the long-term replicability of a benchmark into a tractable problem is linked to choosing a suitable software management system. Several software management systems have emerged to address these problems, from containerization (e.g., apptainer or singularity) \cite{Kurtzer2017-mn} and automation of reproducible and efficient software management (\texttt{EasyBuild} \cite{Hoste2012-gg} or \texttt{Spack} \cite{Gamblin2015-ll}) to distributing software installations such as CernVM-FS \cite{Blomer2013-jk},  or combinations of the two such as \texttt{computecanada} \cite{Boissonneault2019-wm} and \texttt{EESSI} \cite{Droge2023-ax}.

\end{enumerate}

\section*{Software licenses and attribution}

Even the simplest benchmark designed and run by a single person aiming to compare their method against various `gold standard' methods incorporates the work of many (dataset generators, method developers, operating systems with system dependencies and their developers). Benchmark artifacts are generated by running a workflow \cite{Jackson2021-ke}. Hence, data, code and operating system dependencies are all accessible during the benchmarking process. For reproducibility and transparency and to build trust \cite{Laine2007-py}, this freedom should be extended for others to run (re-run), copy (including fork), distribute (e.g., snapshot), study (e.g., read how another person is implementing my own method), change (bug fix, contribute) and improve (extend) benchmarks. Such freedoms are granted (or restricted) by software and content licenses, as are copyright and authorship and attribution \cite{Kreutzer2014-ua}. Without explicit licenses, even publicly available code is copyrighted by default, and reuse is restricted (to reproduce, distribute, or create derivative works) \cite{Kreutzer2014-ua}. 

In practical terms, a benchmarking system needs to be aware of software and data licenses, track further changes and attribute them to the (academic) benchmark contributor. A concise summary of license rights and restrictions, and compatibility across commonly-used open-source licenses can be found in Table~\ref{tab:licenses}. Benchmark contributions should bring a clear FOSS (free and open source software) license, perhaps by assigning the copyright holder to contributors describing their name, email and ORCID (as the canonical scholarly identifier), hence allowing seamless reuse, transparency and easing attribution.

\begin{table}[]
    \caption[Overview of commonly-used open-source licenses in benchmarking studies.]{Overview of commonly used open-source licenses in benchmarking studies. Code or data with restrictions to redistribute or modify are not suitable for collaborative benchmarking. \emph{License}: license name. \emph{Redistribution}: right to redistribute, e.g. reuse in another benchmark/somewhere else. \emph{Modification}: right to modify the code to produce derivative works, e.g. in another benchmark/somewhere else. \emph{Sublicensing}: for derivative works, obligation to keep the same license or freedom to be licensed differently (even copyright). Relevant if the code contributor does not want derivative works to be `closed source' or copyrighted. \emph{Commercial use}: for derivative works, whether commercial usage is restricted or not. Relevant to code contributors not willing commercial reuse of their code (by others). \emph{Attribution}: for derivative works, obligation to attribute original author(s). Relevant to code contributors willing their names/authorship to be credited in derivative works. \emph{Linking}: dynamic or static linking / use as libraries. Particularly important for metrics, as these code can be frequently incorporated (=executed) across benchmarks. }
    \vspace{0.5cm}
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    License       &  Redistribution       &   Modification   & Sublicensing  & Commercial use & Attribution    & Linking   \\ \hline
    no license	& not allowed	& not allowed	& no sublicensing	& not allowed	& required	& not allowed	 \\
    GPLv3 &   	allowed	 & with restrictions	& GPLv3-compatible	& allowed	& required	& GPLv3-compatible \\
    Apache &	allowed &	allowed	& minimal restrictions	& allowed	& required	& allowed \\
    MIT	& allowed	& allowed	& full freedom	& allowed	& required	& allowed	 \\
    Public Domain	& allowed	& allowed	& full freedom	& allowed	& not required	& allowed  \\
    \textsc{CC-BY-NC-ND}	& allowed	& not allowed&	(no derivatives)	& not allowed	& required &	(not a code license)	 \\
    \textsc{CC-BY-SA-NC}	& allowed	& allowed	& not more restrictive& 	not allowed &	required	& (not a code license)	 \\
    \textsc{CC-BY-SA}	& allowed	& allowed	& not more restrictive &	allowed	& required&	(not a code license)  \\
    \textsc{CC-BY} &	allowed	& allowed	& full freedom 	& allowed& 	required&	(not a code license) \\ \hline

    \end{tabular}
    \end{adjustbox}
    \label{tab:licenses}
\end{table}

%https://docs.google.com/spreadsheets/d/1HvhiDHkY5BSvIJ4607krXIiymk9uGWKsEg3cwaZVfBI/edit?gid=0#gid=0 

Similarly, even if all benchmark code were free software, running it could be hindered by (non-free) proprietary interpreters (e.g., MATLAB). Benchmarking systems should be explicit in allowing or disallowing proprietary software requirements, and be aware and track EULAs (end user license agreements). This conundrum extends to the operating system layer and to system dependencies (e.g., compilers). In terms of benchmark re-runnability, fully FOSS system dependencies arguably offer the highest reusability, and can be handled in a reproducible manner across operating systems and system architectures (e.g., amd64, arm64) \cite{Droge2023-ax}.

Finally, data (inputs and outputs) are frequently stored in centralized data repositories \cite{Potter2015-fs, Sicilia2017-ko, van-de-Sandt2019-wd} under content licenses, which might enforce attribution or copyleft. A benchmarking system should automate license metadata handling during data import and export, crediting the original authors if applicable; it also represents an opportunity to enforce that academics use appropriate licenses for data and code. 

Scientific benchmarking is rarely a single-person endeavour and has to deal with diverse content and contributions. Software and content licenses standardize the (legally binding) usage rights and restrictions. Benchmarking systems can leverage licenses and version control to track, store and monitor the reuse of data and code in an automated way. On the other hand, contribution attribution in the academic sense (e.g., authorship) is as important and beyond automation. Collaborative benchmarking initiatives should explicitly list guidelines on using and distributing code and results (i.e., licenses) as well as clearly communicate the authorship strategies. These guidelines, complemented with a code of conduct, could be bundled into the benchmark description. 

\section*{There are various hidden design tradeoffs}

So far, no single benchmarking system can accommodate all use-cases in bioinformatics. However, documentation of design choices for a given system can help to align expectations. Some important design trade-offs are:

\begin{itemize}
    \item Flexibility vs. complexity: unrestricted freedom for method contributors can mean displacing the burden of maintainability towards the benchmarker. Fewer restrictions (e.g., simpler entry points) means that more methods can be included in a single benchmark; but, it can also imply a higher complexity for the initial setup by the benchmarker. Constraints can also enable validation, which can greatly help with the development process and quality assurance.
    \item  Perfect software reproducibility is possible \cite{Lamb2021-tb}, but it comes with a high engineering cost. It is thus reasonable for a benchmarker to state the desired degree of replicability and the corresponding constraints; different tasks might demand different levels of compliance. 
    \item  Security concerns: decentralized runs lower the participation barrier, but it also increases the potential for attacks on their computing environment. Sandboxed environments (restricting network access, filesystem access etc) can be provided as mitigation. Additionally, static and dynamic code analysis in the continuous integration process can aid in approving potentially risky submissions.
\end{itemize}


\section*{Open data formats and standards}

Benchmarking involves applying methods to data. The types of data can be diverse and even a single dataset can be stored in multiple formats. A broad distinction can be made according to general vs. specific formats and open vs. proprietary formats. File formats specific to a programming language should be avoided since those formats may change over time, security risks can arise \cite{Huynh2023-pq, Bleih2024-lv}, and in general they are not interoperable. Likewise, proprietary formats should be avoided, since they may change and could require specific (proprietary) software to read. General language-agnostic formats often do not change over time and are thus more interoperable such that these datasets can be made FAIR \cite{Wilkinson2016-bh}. Generally, using established standards is recommended (e.g., SAM, BED, FASTQ, etc for genomics), since related methods are developed around these (typically storage-efficient) formats.

If raw input data consists of multiple file formats, an option could be to convert the data into a common format with the caveat that this `intermediate' raw data could be either stored (increased storage usage, easier benchmark definition) or created `on the fly' (increased compute). Furthermore, how raw input data is retrieved (e.g., automatic retrieval once and store locally, automatic retrieval every time, or manual retrieval and store locally) needs to be decided.

Additionally, which level and scope of data validation needs to be decided. Validations include content integrity (e.g., md5sum), adherence to a data specification (e.g., JSON Schema), and data semantics (e.g., SHACL for linked data). Intermediate data and results should be made FAIR, leading to additional decisions, including the scope of data sharing (e.g., only the most important subset), the handling of the metadata (automatic versus manual annotation) and the data storage provider and policies (where and how to make available, e.g., GEO or Zenodo). Automatic generation of metadata might require predefined standards and an ontology to fully track how the data was generated \cite{LeboUnknown-om}. 


\section*{Conclusion}
\label{sec:conclusion}

Benchmarking is a cornerstone practice in bioinformatics, impacting how methods are conceived, developed, published and chosen. Benchmarking is driven by the effort of people with the expertise to address (biological and technical) problems. This expertise can be leveraged by current practices of community benchmarking, so scientific advance is eased by joint efforts to catalog open problems, generate (or simulate) appropriate input data, develop methods to address them, and evaluate their performances. Community benchmarking has been proved successful and very productive \cite{Moult2005-ne,Capella-Gutierrez2017-dh,Luecken2024-fk}.

Here we discussed the motivation and process of community benchmarking at several levels, from definition to execution, while highlighting technologies to potentially ease the technical, scientific and contribution challenges. 

\hfill

\section*{Acknowledgments} 

We thank the Robinsonlab members for their constructive feedback.

\section*{Author contributions}

Conceptualization: all. Visualization: BC. Original draft preparation: all. Writing, reviewing and editing: all. Supervision: IM, MDR. Funding acquisition: MDR.

\section*{Funding}

MDR acknowledges funding from the Swiss National Science Foundation (grants 200021\_212940 and 310030\_204869) as well as support from swissuniversities P5 Phase B funding (project 23-36\_14). CS is supported by the Novartis Research Foundation.

\section*{Disclosure statement}

No potential conflict of interest is reported by the authors

\pagebreak
\linespread{1.1} 
\printbibliography

\end{document}
